#!/bin/bash

remove-inline-js() {
  HTML_FILE="$1"
  HTML_FILE_BAK="$HTML_FILE.BAK"
  cp "$HTML_FILE" "$HTML_FILE_BAK"

  xidel -s "$HTML_FILE_BAK" -e 'x:replace-nodes(//body/script,())' --output-format=html >"$HTML_FILE"
  #  rm "$HTML_FILE_BAK"
}

remove-inline-js-all-html-files() {
  [ -z "$1" ] && echo "no directory supplied to function 'remove-inline-js-all-html-files'" && return 1

  while read -r FILE; do
    remove-inline-js "$FILE"
  done < <(find "$1" -iwholename "*/p/*.html")  # doesn't work, only the actual post file alone should be modified
}

extract_substack_post_slug() {
  python -c 'print("'"$1"'".split("/")[-1])'
}

# https://micronaut.substack.com/p/the-ak-snowmelt
scrape-substack-page() {
  URL="$(echo -n "$1")"

  DIR="$substack_pages_dir"
  [ -n "$2" ] && DIR="$(realpath "$2")"
  ! [ -d "$DIR" ] && echo "not a directory '$DIR', abort" && return

  DOMAIN="$(extract_domain_name "$URL")"
  DOMAIN="$(remove_trailing_slash "$DOMAIN")"
  POST="$(extract_substack_post_slug "$URL")" # last segment of the url

  POST_DIR="$DIR/$DOMAIN/$POST"

  # [Nice command for HTTrack](https://gist.github.com/tmslnz/69d08ada96a66a39b463bc5824f2ea75)
  echo "start scraping ${URL}..."
  httrack "$URL" -O "${POST_DIR}" \
    -N100 −%i0 -I0 --max-rate 0 --disable-security-limits --near -v

  echo "removing inline javascript (blocks offline reading)..."
  POST_HTML_FILE="$(realpath "$(find "$DIR" -iwholename "*/${POST}.html")")"
  remove-inline-js "$POST_HTML_FILE" # creates a backup file: $POST_HTML_FILE.BAK

  ln -s "$POST_HTML_FILE" "$DIR/$DOMAIN/${POST}.html"
}

# broken, scrape with selenium or similar
scrape-substack-pages() {
  echo "broken function: scrape is blocked by popup on author homepage with a request to subscribe" && return
  URL="$(echo -n "$1")"

  DIR="$substack_pages_dir"
  [ -n "$2" ] && DIR="$(realpath "$2")"
  ! [ -d "$DIR" ] && echo "not a directory '$DIR', abort" && return

  DOMAIN="$(extract_domain_name "$URL")"
  DOMAIN="$(remove_trailing_slash "$DOMAIN")"
#  POST="$(extract_substack_post_slug "$URL")" # last segment of the url

  POST_DIR="$DIR/$DOMAIN"

  # [Nice command for HTTrack](https://gist.github.com/tmslnz/69d08ada96a66a39b463bc5824f2ea75)
  echo "start scraping ${URL}..."
  httrack "$URL" -O "${POST_DIR}" \
    -N100 −%i0 -I0 --max-rate 0 --disable-security-limits --near -v

  echo "removing inline javascript (blocks offline reading)..."
  rmjs "$POST_DIR"
#  POST_HTML_FILE="$(realpath "$(find "$DIR" -iwholename "*/${POST}.html")")"
#  remove-inline-js "$POST_HTML_FILE" # creates a backup file: $POST_HTML_FILE.BAK

#  ln -s "$POST_HTML_FILE" "$DIR/$DOMAIN/${POST}.html"
  echo "scraping done"
}
